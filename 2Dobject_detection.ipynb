{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2Dobject_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO4PvZ0JHrJt",
        "colab_type": "text"
      },
      "source": [
        "# Create Dataset object \n",
        "- first list all image(.png) and label(.txt) files \n",
        "- augment images to create more variability in training data \n",
        "    - essential to not overfit model \n",
        "- get image, bounding box info, and label from files \n",
        "    - target must be in form: target={'boxes': xxx 'labels': xxx} \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U00FBKhew_8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "from torchvision import transforms\n",
        "from PIL import Image \n",
        "import os \n",
        "import random \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#####################################\n",
        "# image augmentation library \n",
        "# https://github.com/aleju/imgaug\n",
        "# pip install git+https://github.com/aleju/imgaug.git\n",
        "#####################################\n",
        "import imgaug as ia \n",
        "import imgaug.augmenters as iaa \n",
        "\n",
        "class KITTI(Dataset): \n",
        "    def __init__(self, basedir, transforms=transforms.ToTensor()): \n",
        "        self.basedir = basedir\n",
        "        self.transforms = transforms \n",
        "\n",
        "        # get list of filenames         \n",
        "        self.images = [file for file in os.listdir(self.basedir + '/training/image_2') if file.endswith('.png')]\n",
        "        self.labels = [file for file in os.listdir(self.basedir + '/label_2') if file.endswith('.txt')]\n",
        "        self.images.sort()\n",
        "        self.labels.sort()\n",
        "        \n",
        "        # class type to integer \n",
        "        self.CLASS = {'Car':0, 'Van':1, 'Truck':2, 'Pedestrian':3, 'Person_sitting':4, 'Cyclist':5, 'Tram':6, 'Misc':7, 'DontCare':8}\n",
        "        self.classes = ['Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc', 'DontCare']\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.labels)        \n",
        "\n",
        "    def augment(self, image): \n",
        "        '''\n",
        "        Augments a single image \n",
        "        Args \n",
        "            image: PIL image ---> convert TO numpy \n",
        "        Returns\n",
        "            aug: PIL image ----> converted FROM numpy \n",
        "        ''' \n",
        "        image = np.asarray(image) # convert PIL image to numpy array \n",
        "        rand_int = random.randint(-5,5)\n",
        "        value = 0 if rand_int < 0 else rand_int\n",
        "        seq = iaa.Sequential([\n",
        "                iaa.SomeOf((0, 2)),\n",
        "                    iaa.Emboss(alpha=(0, 1.0), strength=(0, 0.75)), # emboss images\n",
        "                    iaa.OneOf([\n",
        "                        iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 3.0\n",
        "                        iaa.AverageBlur(k=(5, 7)), # blur image using local means with kernel sizes between 5 and 7\n",
        "                        iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 3 and 11\n",
        "                    ]),\n",
        "                \n",
        "                    iaa.OneOf([\n",
        "                        # either change the brightness of the whole image (sometimes\n",
        "                        # per channel) or change the brightness of subareas\n",
        "                        iaa.Multiply((0.8, 1.2), per_channel=0.5),\n",
        "                        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n",
        "                    ]),\n",
        "                \n",
        "                    iaa.OneOf([\n",
        "                        iaa.Dropout(p=0.05, per_channel=True),\n",
        "                        iaa.Crop(px=(0, value)), # crop images from each side by 0 to 4px (randomly chosen)\n",
        "                    ])\n",
        "            ])\n",
        "        \n",
        "        img = seq.augment_image(image) \n",
        "        img = Image.fromarray(img.astype('uint8'), 'RGB') # convert numpy array to PIL image \n",
        "        return img \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = Image.open(self.basedir + '/training/image_2/' + self.images[idx])\n",
        "        label = open(self.basedir + '/label_2/' + self.labels[idx],'r')\n",
        "        \n",
        "        # Get the image \n",
        "        image = self.augment(image)\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)  \n",
        "\n",
        "        # Get the bounding box and class types \n",
        "        Lines = label.readlines() # read lines in txt file one by one\n",
        "        bbox = [] \n",
        "        types = []\n",
        "        for i, line in enumerate(Lines): \n",
        "            elem = line.split()\n",
        "            if i == 0: \n",
        "                left = float(elem[4]) \n",
        "                top = float(elem[5])\n",
        "                right = float(elem[6])\n",
        "                bottom = float(elem[7])\n",
        "                bbox = np.array([left, top, right, bottom])\n",
        "            else: \n",
        "                left = float(elem[4]) \n",
        "                top = float(elem[5])\n",
        "                right = float(elem[6])\n",
        "                bottom = float(elem[7])\n",
        "                curr_box = np.array([left, top, right, bottom])\n",
        "                bbox = np.vstack((bbox, curr_box))\n",
        "            label_type = elem[0]\n",
        "            label_type = self.CLASS.get(label_type)\n",
        "            types.append(label_type)\n",
        "\n",
        "        # convert to tensor  \n",
        "        boxes = torch.as_tensor(bbox, dtype = torch.float32)\n",
        "        labels = torch.as_tensor(types, dtype=torch.int64)\n",
        "        \n",
        "        # get target into right format \n",
        "        target = {} \n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels \n",
        "        return (image, target)\n",
        "        \n",
        "\n",
        "# ### Testing ### \n",
        "# basedir = '/content/drive/My Drive/Personal/2Dobject_detection/data'\n",
        "# ds = KITTI(basedir)\n",
        "# a = ds.__len__()\n",
        "# b = ds.__getitem__(65)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De-lrBxwNcC1",
        "colab_type": "text"
      },
      "source": [
        "# Finetune a pretrained model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYFaMI5MHv1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = 9 \n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVg4PulCOR9F",
        "colab_type": "text"
      },
      "source": [
        "# Train model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM3spgZtOUqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim \n",
        "\n",
        "basedir = '/content/drive/My Drive/Personal/2Dobject_detection/data'\n",
        "ds = KITTI(basedir)\n",
        "train_loader = DataLoader(dataset=ds, batch_size=1, shuffle=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=.001)\n",
        "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwznupR7YPFj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9f83080-5a8c-4db1-cf6c-6de3cf5f0830"
      },
      "source": [
        "from tqdm import tqdm \n",
        "\n",
        "def train_one_epoch(model, optimizer, device): \n",
        "    for i,(images, targets) in enumerate(train_loader): \n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        images = [image.to(device) for image in images]\n",
        "\n",
        "        #### the dataloader puts the dict values into a different shape then \n",
        "        #### what I created them to be. Wonder why? \n",
        "        #### maybe because I have batch_size == 1: \n",
        "        #### stacking images caused issues; maybe because different sizes? \n",
        "        #### documentation says images can be different sizes with collate_fn::::: TO DO \n",
        "    \n",
        "        if targets['boxes'].dim() == 3: \n",
        "            size = targets['boxes'].shape\n",
        "            targets['boxes'] = targets['boxes'].reshape((size[1], size[2]))\n",
        "        if targets['labels'].dim() == 2: \n",
        "            targets['labels'] = targets['labels'].reshape((targets['labels'].shape[1]))\n",
        "        \n",
        "        targets = [{k:v.to(device) for k,v in targets.items()}]\n",
        "        # print(targets['labels'])\n",
        "        # print(targets['labels'].shape)\n",
        "        output = model(images, targets)\n",
        "        losses = sum(loss for loss in output.values())\n",
        "        losses.backward() \n",
        "        optimizer.step()\n",
        "        if i % 50 == 0: \n",
        "            print('Loss: {}'.format(losses))\n",
        "    return output \n",
        "\n",
        "\n",
        "\n",
        "def train(model, optimizer, device, num_epochs=10): \n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(1, num_epochs+1)): \n",
        "        output = train_one_epoch(model,optimizer, device)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(model, optimizer, device, num_epochs=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1.2268264293670654\n",
            "Loss: 0.19299210608005524\n",
            "Loss: 0.6901226043701172\n",
            "Loss: 0.42334648966789246\n",
            "Loss: 0.6255536675453186\n",
            "Loss: 0.6094328761100769\n",
            "Loss: 2.276123523712158\n",
            "Loss: 5.634336471557617\n",
            "Loss: 1.792634129524231\n",
            "Loss: 9.558202743530273\n",
            "Loss: 89.03678894042969\n",
            "Loss: 8.754332542419434\n",
            "Loss: 43.03195571899414\n",
            "Loss: 1.3514418601989746\n",
            "Loss: 0.7985383868217468\n",
            "Loss: 0.7064328789710999\n",
            "Loss: 0.6408401727676392\n",
            "Loss: 0.6467110514640808\n",
            "Loss: 0.9968171119689941\n",
            "Loss: 0.5014932751655579\n",
            "Loss: 1.2766413688659668\n",
            "Loss: 0.5586798191070557\n",
            "Loss: 0.6295728087425232\n",
            "Loss: 0.5943513512611389\n",
            "Loss: 0.47257745265960693\n",
            "Loss: 0.5654472708702087\n",
            "Loss: 0.6523834466934204\n",
            "Loss: 0.6639809012413025\n",
            "Loss: 0.8757748007774353\n",
            "Loss: 0.47696396708488464\n",
            "Loss: 0.7971283197402954\n",
            "Loss: 0.6320591568946838\n",
            "Loss: 0.3628688454627991\n",
            "Loss: 0.4482140839099884\n",
            "Loss: 0.5003513097763062\n",
            "Loss: 0.3706916570663452\n",
            "Loss: 0.7949826717376709\n",
            "Loss: 0.3964989185333252\n",
            "Loss: 0.38971754908561707\n",
            "Loss: 0.48299115896224976\n",
            "Loss: 0.5939031839370728\n",
            "Loss: 0.3211476504802704\n",
            "Loss: 0.8441914319992065\n",
            "Loss: 0.5270803570747375\n",
            "Loss: 0.7642452716827393\n",
            "Loss: 0.3362947106361389\n",
            "Loss: 0.5025138854980469\n",
            "Loss: 0.3748246431350708\n",
            "Loss: 0.5483760833740234\n",
            "Loss: 0.7370312809944153\n",
            "Loss: 0.42667943239212036\n",
            "Loss: 0.8746678829193115\n",
            "Loss: 1.6231844425201416\n",
            "Loss: 1.4214786291122437\n",
            "Loss: 0.6871703863143921\n",
            "Loss: 0.6507869958877563\n",
            "Loss: 0.29890745878219604\n",
            "Loss: 0.5888769030570984\n",
            "Loss: 0.7616172432899475\n",
            "Loss: 1.2575562000274658\n",
            "Loss: 0.6975980401039124\n",
            "Loss: 1.1925089359283447\n",
            "Loss: 0.5717741250991821\n",
            "Loss: 0.2852723002433777\n",
            "Loss: 0.3778788447380066\n",
            "Loss: 0.3529939651489258\n",
            "Loss: 0.500336766242981\n",
            "Loss: 0.8980369567871094\n",
            "Loss: 0.64882493019104\n",
            "Loss: 0.3768478333950043\n",
            "Loss: 0.6940920352935791\n",
            "Loss: 0.2948407530784607\n",
            "Loss: 0.46860185265541077\n",
            "Loss: 0.2564851939678192\n",
            "Loss: 0.5085877180099487\n",
            "Loss: 0.3906499743461609\n",
            "Loss: 0.7709853649139404\n",
            "Loss: 0.7016425132751465\n",
            "Loss: 0.5384841561317444\n",
            "Loss: 0.5120512843132019\n",
            "Loss: 0.49227333068847656\n",
            "Loss: 1.6580092906951904\n",
            "Loss: 0.5550967454910278\n",
            "Loss: 1.4368035793304443\n",
            "Loss: 0.777319073677063\n",
            "Loss: 0.37574508786201477\n",
            "Loss: 0.7793538570404053\n",
            "Loss: 0.22976593673229218\n",
            "Loss: 0.47755664587020874\n",
            "Loss: 0.4771084487438202\n",
            "Loss: 0.3142363727092743\n",
            "Loss: 0.7905102968215942\n",
            "Loss: 0.6301024556159973\n",
            "Loss: 1.1097618341445923\n",
            "Loss: 0.5831000804901123\n",
            "Loss: 0.745144248008728\n",
            "Loss: 0.26305845379829407\n",
            "Loss: 0.7266620397567749\n",
            "Loss: 0.3986576199531555\n",
            "Loss: 0.29054713249206543\n",
            "Loss: 0.2161746472120285\n",
            "Loss: 0.2513897120952606\n",
            "Loss: 0.4426756501197815\n",
            "Loss: 0.292668879032135\n",
            "Loss: 0.4673670530319214\n",
            "Loss: 0.38834282755851746\n",
            "Loss: 1.1864031553268433\n",
            "Loss: 1.2126647233963013\n",
            "Loss: 0.23154614865779877\n",
            "Loss: 0.5665022134780884\n",
            "Loss: 0.5195029973983765\n",
            "Loss: 1.5154333114624023\n",
            "Loss: 0.4872814416885376\n",
            "Loss: 0.1982305496931076\n",
            "Loss: 1.117976188659668\n",
            "Loss: 0.24689669907093048\n",
            "Loss: 0.4490637481212616\n",
            "Loss: 0.35089311003685\n",
            "Loss: 0.6695202589035034\n",
            "Loss: 0.28645431995391846\n",
            "Loss: 0.9120025038719177\n",
            "Loss: 0.4349938929080963\n",
            "Loss: 0.2961179316043854\n",
            "Loss: 1.8412764072418213\n",
            "Loss: 0.3944857120513916\n",
            "Loss: 1.312300443649292\n",
            "Loss: 1.0105137825012207\n",
            "Loss: 0.44933021068573\n",
            "Loss: 0.6548939943313599\n",
            "Loss: 1.5151925086975098\n",
            "Loss: 0.3937147855758667\n",
            "Loss: 0.9363360404968262\n",
            "Loss: 0.23067857325077057\n",
            "Loss: 0.36590126156806946\n",
            "Loss: 0.3545413315296173\n",
            "Loss: 0.20933422446250916\n",
            "Loss: 0.45294538140296936\n",
            "Loss: 0.4151342511177063\n",
            "Loss: 0.8655169010162354\n",
            "Loss: 0.43145090341567993\n",
            "Loss: 0.8966237306594849\n",
            "Loss: 0.42794322967529297\n",
            "Loss: 0.35867294669151306\n",
            "Loss: 0.4353626072406769\n",
            "Loss: 0.5456724762916565\n",
            "Loss: 0.19824454188346863\n",
            "Loss: 0.499481737613678\n",
            "Loss: 0.33092746138572693\n",
            "Loss: 0.5648612380027771\n",
            "Loss: 0.5480616092681885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 1/15 [6:31:53<91:26:26, 23513.35s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.34173908829689026\n",
            "Loss: 0.24417048692703247\n",
            "Loss: 1.0455904006958008\n",
            "Loss: 0.5900596380233765\n",
            "Loss: 0.8083372116088867\n",
            "Loss: 0.24932146072387695\n",
            "Loss: 0.4139249324798584\n",
            "Loss: 0.42718467116355896\n",
            "Loss: 0.3828098773956299\n",
            "Loss: 0.640572726726532\n",
            "Loss: 0.2501707673072815\n",
            "Loss: 1.2918858528137207\n",
            "Loss: 0.6096632480621338\n",
            "Loss: 0.9390166401863098\n",
            "Loss: 0.4835008978843689\n",
            "Loss: 1.6715396642684937\n",
            "Loss: 0.3358192443847656\n",
            "Loss: 0.26861003041267395\n",
            "Loss: 0.8295992612838745\n",
            "Loss: 0.821998119354248\n",
            "Loss: 0.7044740915298462\n",
            "Loss: 0.23923178017139435\n",
            "Loss: 0.37356552481651306\n",
            "Loss: 0.5223569869995117\n",
            "Loss: 0.22065499424934387\n",
            "Loss: 0.44175073504447937\n",
            "Loss: 1.2367305755615234\n",
            "Loss: 0.5988364219665527\n",
            "Loss: 0.5182716846466064\n",
            "Loss: 1.068030834197998\n",
            "Loss: 0.30778953433036804\n",
            "Loss: 0.7365075945854187\n",
            "Loss: 0.2532760798931122\n",
            "Loss: 0.6996801495552063\n",
            "Loss: 0.24263402819633484\n",
            "Loss: 0.3704892098903656\n",
            "Loss: 0.9332658052444458\n",
            "Loss: 0.3760693073272705\n",
            "Loss: 0.5687687397003174\n",
            "Loss: 1.8603568077087402\n",
            "Loss: 0.7009416818618774\n",
            "Loss: 1.151561975479126\n",
            "Loss: 0.5545719265937805\n",
            "Loss: 0.7564883232116699\n",
            "Loss: 0.41196542978286743\n",
            "Loss: 0.2453424483537674\n",
            "Loss: 0.7466521263122559\n",
            "Loss: 0.27776119112968445\n",
            "Loss: 0.34033602476119995\n",
            "Loss: 0.36333203315734863\n",
            "Loss: 0.4819260239601135\n",
            "Loss: 0.24325792491436005\n",
            "Loss: 0.6830841302871704\n",
            "Loss: 0.34593790769577026\n",
            "Loss: 0.2853690981864929\n",
            "Loss: 0.5074482560157776\n",
            "Loss: 0.26551324129104614\n",
            "Loss: 0.6158257722854614\n",
            "Loss: 0.29261988401412964\n",
            "Loss: 0.41869112849235535\n",
            "Loss: 0.5573878884315491\n",
            "Loss: 0.2157563865184784\n",
            "Loss: 0.8005395531654358\n",
            "Loss: 0.6395256519317627\n",
            "Loss: 0.28677791357040405\n",
            "Loss: 0.25997862219810486\n",
            "Loss: 1.6278021335601807\n",
            "Loss: 0.22238466143608093\n",
            "Loss: 0.858142077922821\n",
            "Loss: 0.6350669264793396\n",
            "Loss: 0.313292533159256\n",
            "Loss: 0.9957253932952881\n",
            "Loss: 0.5796270370483398\n",
            "Loss: 0.26418185234069824\n",
            "Loss: 0.40593457221984863\n",
            "Loss: 0.6022986769676208\n",
            "Loss: 0.18847021460533142\n",
            "Loss: 0.36626356840133667\n",
            "Loss: 0.24235303699970245\n",
            "Loss: 0.3549787402153015\n",
            "Loss: 0.5844063758850098\n",
            "Loss: 0.3877171277999878\n",
            "Loss: 0.3278559148311615\n",
            "Loss: 0.7533592581748962\n",
            "Loss: 2.3646585941314697\n",
            "Loss: 0.5452609062194824\n",
            "Loss: 0.20191752910614014\n",
            "Loss: 0.8945439457893372\n",
            "Loss: 0.8336083292961121\n",
            "Loss: 1.8728113174438477\n",
            "Loss: 0.5557039976119995\n",
            "Loss: 1.2690221071243286\n",
            "Loss: 0.35238322615623474\n",
            "Loss: 2.021045207977295\n",
            "Loss: 0.8530793190002441\n",
            "Loss: 0.502500593662262\n",
            "Loss: 0.3894317150115967\n",
            "Loss: 0.2111126184463501\n",
            "Loss: 0.3226515054702759\n",
            "Loss: 1.3068699836730957\n",
            "Loss: 0.6883695721626282\n",
            "Loss: 0.4811694324016571\n",
            "Loss: 1.3862258195877075\n",
            "Loss: 0.3686171770095825\n",
            "Loss: 0.20955678820610046\n",
            "Loss: 0.49478572607040405\n",
            "Loss: 0.2511085867881775\n",
            "Loss: 0.7310007214546204\n",
            "Loss: 0.8888701796531677\n",
            "Loss: 0.34183549880981445\n",
            "Loss: 0.2384207546710968\n",
            "Loss: 0.9236409068107605\n",
            "Loss: 0.37765663862228394\n",
            "Loss: 1.9425523281097412\n",
            "Loss: 0.4718283414840698\n",
            "Loss: 0.3303110897541046\n",
            "Loss: 0.4982214868068695\n",
            "Loss: 0.6856207847595215\n",
            "Loss: 0.9163224101066589\n",
            "Loss: 0.34517520666122437\n",
            "Loss: 0.34102028608322144\n",
            "Loss: 0.6683015823364258\n",
            "Loss: 0.46098625659942627\n",
            "Loss: 0.6053838133811951\n",
            "Loss: 0.2100953310728073\n",
            "Loss: 0.4594096541404724\n",
            "Loss: 0.23678502440452576\n",
            "Loss: 0.5097893476486206\n",
            "Loss: 0.9170093536376953\n",
            "Loss: 1.1393073797225952\n",
            "Loss: 0.5570287704467773\n",
            "Loss: 1.0361289978027344\n",
            "Loss: 0.4317888021469116\n",
            "Loss: 0.6156343221664429\n",
            "Loss: 0.8130197525024414\n",
            "Loss: 0.23374666273593903\n",
            "Loss: 0.6672376990318298\n",
            "Loss: 0.4139535129070282\n",
            "Loss: 0.9909942746162415\n",
            "Loss: 0.2856689393520355\n",
            "Loss: 0.5423455238342285\n",
            "Loss: 0.30667805671691895\n",
            "Loss: 2.056525230407715\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}